{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Analysis Notebook\n",
    "\n",
    "You are welcome to either work locally or in this Google Colab notebook. Below you will find code to replicate creating the dataset and some code to start you exploring NER and TF-IDF. Please reach out to the instructors if you have questions or concerns. To use this notebook, you will need to install the following packages:\n",
    "\n",
    "```bash\n",
    "!pip install pandas\n",
    "!pip install tqdm\n",
    "!pip install altair\n",
    "!pip install scikit-learn\n",
    "!pip install requests\n",
    "!pip install gutenbergpy\n",
    "!pip install -U spacy\n",
    "!pip install -U spacy-transformers\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download xx_ent_wiki_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import altair as alt\n",
    "import gutenbergpy.textget\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load Spacy's multilingual model (can be replaced with a larger model if needed)\n",
    "multi_nlp = spacy.load('xx_ent_wiki_sm')\n",
    "eng_nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "\n",
    "You either have the option of using the premade dataset available in Google Drive (though you will need to change the path to the file) or running the code below to remake the dataset from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Drive Dataset\n",
    "\n",
    "You can download this dataset here [https://drive.google.com/file/d/1LkaRtYph_lWtMPRyzZpECuzEMD3WPx26/view?usp=sharing](https://drive.google.com/file/d/1LkaRtYph_lWtMPRyzZpECuzEMD3WPx26/view?usp=sharing) and it's very larger so make sure you don't push it up to GitHub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_novels_nyt_df = pd.read_csv(\"combined_novels_nyt_with_text.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerun Dataset Creation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novels_df = pd.read_csv(\"https://raw.githubusercontent.com/melaniewalsh/responsible-datasets-in-context/main/datasets/top-500-novels/library_top_500.csv\")\n",
    "nyt_bestsellers_df = pd.read_csv(\"https://raw.githubusercontent.com/ecds/post45-datasets/main/nyt_full.tsv\", sep='\\t')\n",
    "combined_novels_nyt_df = novels_df.merge(nyt_bestsellers_df, how='left', on=['author', 'title'])\n",
    "\n",
    "def get_text(url):\n",
    "\tif pd.notna(url):\n",
    "\t\ttry:\n",
    "\t\t\tresponse = requests.get(url)\n",
    "\t\t\tif response.status_code == 200:\n",
    "\t\t\t\treturn response.text\n",
    "\t\texcept Exception as e:\n",
    "\t\t\treturn None\n",
    "\t\t\t\n",
    "\treturn None\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas(desc=\"Progress\")\n",
    "combined_novels_nyt_df.loc[:, 'pg_eng_text'] = combined_novels_nyt_df.pg_eng_url.progress_apply(get_text)\n",
    "combined_novels_nyt_df.loc[:, 'pg_orig_text'] = combined_novels_nyt_df['pg_orig_url'].progress_apply(get_text)\n",
    "\n",
    "combined_novels_nyt_df['pg_eng_tokens'] = combined_novels_nyt_df['pg_eng_text'].str.split()\n",
    "combined_novels_nyt_df['pg_orig_tokens'] = combined_novels_nyt_df['pg_orig_text'].str.split()\n",
    "\n",
    "combined_novels_nyt_df['pg_eng_text_len'] = combined_novels_nyt_df.pg_eng_text.str.len()\n",
    "combined_novels_nyt_df['pg_orig_text_len'] = combined_novels_nyt_df.pg_orig_text.str.len()\n",
    "combined_novels_nyt_df['pg_eng_token_len'] = combined_novels_nyt_df.pg_eng_tokens.str.len()\n",
    "combined_novels_nyt_df['pg_orig_token_len'] = combined_novels_nyt_df.pg_orig_tokens.str.len()\n",
    "\n",
    "def clean_book(url):\n",
    "\t# This gets a book by its gutenberg id number\n",
    "\tif pd.notna(url):\n",
    "\t\tpg_id = url.split('/pg')[-1].split('.')[0]\n",
    "\t\ttry:\n",
    "\t\t\traw_book = gutenbergpy.textget.get_text_by_id(pg_id) # with headers\n",
    "\t\t\tclean_book = gutenbergpy.textget.strip_headers(raw_book) # without headers\n",
    "\t\t\treturn clean_book\n",
    "\t\texcept Exception as e:\n",
    "\t\t\treturn None\n",
    "\n",
    "combined_novels_nyt_df.loc[:, 'cleaned_pg_eng_text'] = combined_novels_nyt_df.pg_eng_url.apply(clean_book)\n",
    "combined_novels_nyt_df.loc[:, 'cleaned_pg_orig_text'] = combined_novels_nyt_df.pg_orig_url.apply(clean_book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER Code\n",
    "\n",
    "There are many libraries for doing NER but today we'll be using `spaCy`, which is one of the most popular. You can visit the documentation here [https://spacy.io/usage](https://spacy.io/usage).\n",
    "\n",
    "Now spaCy is a much more complex library than we have used before so let's try out an example and then talk through some the documentation. Let's start with an example from their spaCy 101 guide [https://spacy.io/usage/spacy-101#annotations-ner](https://spacy.io/usage/spacy-101#annotations-ner) and try copying some code.\n",
    "\n",
    "```python\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "```\n",
    "\n",
    "In our case we'll change `nlp` to be `eng_nlp`. Let's try running this code and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = eng_nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what exactly does this output mean? Well first let's explore a bit of the spaCy API. First how could we check out what our `doc` variable contains?\n",
    "\n",
    "We should see `spacy.tokens.doc.Doc` so let's take a look at the `Doc` <https://spacy.io/api/doc>. The documentation is pretty dense, but we should get a sense that a `Doc` is a collection of [`Token`](https://spacy.io/api/token) classes, and that it has a number of built-in methods. \n",
    "\n",
    "We can list these methods with the following code:\n",
    "\n",
    "```python\n",
    "# Let's dig into what this Class gives us\n",
    "[prop for prop in dir(doc) if not prop.startswith('_')]\n",
    "\n",
    "first_word = doc[0]\n",
    "type(first_word)\n",
    "\n",
    "[prop for prop in dir(first_word) if not prop.startswith('__')]\n",
    "```\n",
    "\n",
    "Take a look at the [`similiarity`](https://spacy.io/api/doc#similarity) and [`ents`](https://spacy.io/api/doc#ents) methods. What do these methods/attributes do? \n",
    "\n",
    "Part of understanding what they are doing, requires understanding how spaCy works. Below is a figure of their pipeline:\n",
    "\n",
    "![spacy pipeline](https://d33wubrfki0l68.cloudfront.net/3ad0582d97663a1272ffc4ccf09f1c5b335b17e9/7f49c/pipeline-fde48da9b43661abcdf62ab70a546d71.svg)\n",
    "\n",
    "This gives us a bit of a sense of how this is working (recognize tokenizer?) but let's also read their broad overview page <https://spacy.io/usage/facts-figures>. \n",
    "\n",
    "Looking at their comparison usage, what do you think are the benefits and limitations of spaCy? How does spaCy create their models and what are these models exactly?\n",
    "\n",
    "Returning our example above, let's try using one of spaCy's built-in visualizations to understand how this is working.\n",
    "\n",
    "```python\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\")\n",
    "```\n",
    "\n",
    "We should now see something that looks like this:\n",
    "\n",
    "![spacy pos](https://spacy.io/displacy-model-rules2-631f1baa2683dc767a822ab48d067678.svg)\n",
    "\n",
    "What this visualization is highlighting is essentially how spaCy works, which is with something called Parts of Speech Tagging.\n",
    "\n",
    "From the [spaCy docs](https://spacy.io/usage/linguistic-features#pos-tagging):\n",
    "\n",
    "> After tokenization, spaCy can parse and tag a given Doc. This is where the trained pipeline and its statistical models come in, which enable spaCy to make predictions of which tag or label most likely applies in this context. A trained component includes binary data that is produced by showing a system enough examples for it to make predictions that generalize across the language – for example, a word following “the” in English is most likely a noun.\n",
    "\n",
    "So the key thing to understand is that this super powerful library also comes with a lot of built-in assumptions about how language in your corpus is structured.\n",
    "\n",
    "Now that we've considered some of the pros and cons, let's try out spaCy's NER with some of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152K)\\n\\n\\nFull 73 88 CARDINAL\n",
      "Miguel de Cervantes\\n\\n\\n\\n Translated 129 167 PERSON\n",
      "John Ormsby\\n\\n\\n\\n\\nEbook 171 197 PERSON\n",
      "Note\\n\\n\\n\\nThe 218 233 ORG\n",
      "Ormsby 320 326 PERSON\n",
      "J. W. Clark 391 402 PERSON\n",
      "Gustave Dor\\xc3\\xa9 419 438 PERSON\n",
      "Clark 440 445 PERSON\n",
      "English 491 498 LANGUAGE\n",
      "\\xe2\\x80\\x98Don Quixote\\xe2\\x80\\x99 507 542 ORG\n"
     ]
    }
   ],
   "source": [
    "text = combined_novels_nyt_df.cleaned_pg_eng_text[0][0:10000]\n",
    "doc = eng_nlp(text)\n",
    "\n",
    "for ent in doc.ents[0:10]:\n",
    "\tprint(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try and take a look at some of our entities. You'll notice previously that in our example there was a number of different entities highlighted. Below we can see the number of entities that exist in spaCy. Let's try and print out just one type of entity initially.\n",
    "\n",
    "![ner entites](https://devopedia.org/images/article/256/8660.1580659054.png)\n",
    "\n",
    "Now let's try out visualizing our entities with the spaCy entity visualizer https://spacy.io/usage/visualizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run TF-IDF we'll be using the `scikit-learn` library. You can find the documentation here [https://scikit-learn.org/stable/](https://scikit-learn.org/stable/). In particular, we'll be using the `TfidfVectorizer` class [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>term</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115304890</th>\n",
       "      <td>The Last Days of Pompeii</td>\n",
       "      <td>the</td>\n",
       "      <td>0.730242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115304767</th>\n",
       "      <td>Germinal</td>\n",
       "      <td>the</td>\n",
       "      <td>0.700247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115304868</th>\n",
       "      <td>Nostromo</td>\n",
       "      <td>the</td>\n",
       "      <td>0.699570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115304932</th>\n",
       "      <td>Death In Venice</td>\n",
       "      <td>the</td>\n",
       "      <td>0.687305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115304523</th>\n",
       "      <td>The Grapes of Wrath</td>\n",
       "      <td>the</td>\n",
       "      <td>0.670729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115304601</th>\n",
       "      <td>The War of the Worlds</td>\n",
       "      <td>the</td>\n",
       "      <td>0.668319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115304796</th>\n",
       "      <td>The Phantom of the Opera</td>\n",
       "      <td>the</td>\n",
       "      <td>0.667619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115304750</th>\n",
       "      <td>Death Comes for the Archbishop</td>\n",
       "      <td>the</td>\n",
       "      <td>0.659896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115304525</th>\n",
       "      <td>The Last of the Mohicans</td>\n",
       "      <td>the</td>\n",
       "      <td>0.659860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115304564</th>\n",
       "      <td>A Journey to the Center of the Earth</td>\n",
       "      <td>the</td>\n",
       "      <td>0.653512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title term     score\n",
       "115304890              The Last Days of Pompeii  the  0.730242\n",
       "115304767                              Germinal  the  0.700247\n",
       "115304868                              Nostromo  the  0.699570\n",
       "115304932                       Death In Venice  the  0.687305\n",
       "115304523                   The Grapes of Wrath  the  0.670729\n",
       "115304601                 The War of the Worlds  the  0.668319\n",
       "115304796              The Phantom of the Opera  the  0.667619\n",
       "115304750        Death Comes for the Archbishop  the  0.659896\n",
       "115304525              The Last of the Mohicans  the  0.659860\n",
       "115304564  A Journey to the Center of the Earth  the  0.653512"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the text data\n",
    "tfidf_matrix = vectorizer.fit_transform(combined_novels_nyt_df.cleaned_pg_eng_text.fillna(''))\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame for better readability\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Add the titles to the DataFrame\n",
    "tfidf_df['title'] = combined_novels_nyt_df['title'].values\n",
    "\n",
    "# Melt the DataFrame to get a long format DataFrame with terms and scores\n",
    "melted_tfidf_df = tfidf_df.melt(id_vars=['title'], var_name='term', value_name='score')\n",
    "\n",
    "# Sort the DataFrame by score in descending order\n",
    "sorted_tfidf_df = melted_tfidf_df.sort_values(by='score', ascending=False)\n",
    "\n",
    "# Display the top 10 results\n",
    "sorted_tfidf_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that TF-IDF has a number of parameters that you can set. Specifically it includes parameters for dealing with stop words, max_df, min_df, and ngram_range. Let's experiment with some of these parameters.\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Let's try out some different parameters\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_df=0.5, min_df=2, ngram_range=(1, 2))\n",
    "tfidf_matrix = tfidf.fit_transform(df['text'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.5, min_df=2, ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform the text data\n",
    "tfidf_matrix = vectorizer.fit_transform(combined_novels_nyt_df.cleaned_pg_eng_text.fillna(''))\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame for better readability\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Add the titles to the DataFrame\n",
    "tfidf_df['title'] = combined_novels_nyt_df['title'].values\n",
    "\n",
    "# Melt the DataFrame to get a long format DataFrame with terms and scores\n",
    "melted_tfidf_df = tfidf_df.melt(id_vars=['title'], var_name='term', value_name='score')\n",
    "\n",
    "# Sort the DataFrame by score in descending order\n",
    "sorted_tfidf_df = melted_tfidf_df.sort_values(by='score', ascending=False)\n",
    "\n",
    "# Display the top 10 results\n",
    "sorted_tfidf_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-work-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
