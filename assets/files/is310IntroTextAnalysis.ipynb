{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Analysis Notebook\n",
    "\n",
    "You are welcome to either work locally or in this Google Colab notebook. Below you will find code to replicate creating the dataset and some code to start you exploring NER and TF-IDF. Please reach out to the instructors if you have questions or concerns. To use this notebook, you will need to install the following packages:\n",
    "\n",
    "```bash\n",
    "!pip install pandas\n",
    "!pip install tqdm\n",
    "!pip install altair\n",
    "!pip install scikit-learn\n",
    "!pip install requests\n",
    "!pip install gutenbergpy\n",
    "!pip install -U spacy\n",
    "!pip install -U spacy-transformers\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download xx_ent_wiki_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import altair as alt\n",
    "import gutenbergpy.textget\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load Spacy's multilingual model (can be replaced with a larger model if needed)\n",
    "multi_nlp = spacy.load('xx_ent_wiki_sm')\n",
    "eng_nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "\n",
    "You either have the option of using the premade dataset available in Google Drive (though you will need to change the path to the file) or running the code below to remake the dataset from scratch. \n",
    "\n",
    "**Be warned, this file is quite large because of the size of the novels, so you may want to use a subset of the novels to test this code.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Drive Dataset\n",
    "\n",
    "You can download this dataset here [https://drive.google.com/file/d/1LkaRtYph_lWtMPRyzZpECuzEMD3WPx26/view?usp=sharing](https://drive.google.com/file/d/1LkaRtYph_lWtMPRyzZpECuzEMD3WPx26/view?usp=sharing) and it's very larger so make sure you don't push it up to GitHub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_novels_nyt_df = pd.read_csv(\"combined_novels_nyt_with_text.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerun Dataset Creation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novels_df = pd.read_csv(\"https://raw.githubusercontent.com/melaniewalsh/responsible-datasets-in-context/main/datasets/top-500-novels/library_top_500.csv\")\n",
    "nyt_bestsellers_df = pd.read_csv(\"https://raw.githubusercontent.com/ecds/post45-datasets/main/nyt_full.tsv\", sep='\\t')\n",
    "combined_novels_nyt_df = novels_df.merge(nyt_bestsellers_df, how='left', on=['author', 'title'])\n",
    "\n",
    "def get_text(url):\n",
    "\tif pd.notna(url):\n",
    "\t\ttry:\n",
    "\t\t\tresponse = requests.get(url)\n",
    "\t\t\tif response.status_code == 200:\n",
    "\t\t\t\treturn response.text\n",
    "\t\texcept Exception as e:\n",
    "\t\t\treturn None\n",
    "\t\t\t\n",
    "\treturn None\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas(desc=\"Progress\")\n",
    "combined_novels_nyt_df.loc[:, 'pg_eng_text'] = combined_novels_nyt_df.pg_eng_url.progress_apply(get_text)\n",
    "combined_novels_nyt_df.loc[:, 'pg_orig_text'] = combined_novels_nyt_df['pg_orig_url'].progress_apply(get_text)\n",
    "\n",
    "combined_novels_nyt_df['pg_eng_tokens'] = combined_novels_nyt_df['pg_eng_text'].str.split()\n",
    "combined_novels_nyt_df['pg_orig_tokens'] = combined_novels_nyt_df['pg_orig_text'].str.split()\n",
    "\n",
    "combined_novels_nyt_df['pg_eng_text_len'] = combined_novels_nyt_df.pg_eng_text.str.len()\n",
    "combined_novels_nyt_df['pg_orig_text_len'] = combined_novels_nyt_df.pg_orig_text.str.len()\n",
    "combined_novels_nyt_df['pg_eng_token_len'] = combined_novels_nyt_df.pg_eng_tokens.str.len()\n",
    "combined_novels_nyt_df['pg_orig_token_len'] = combined_novels_nyt_df.pg_orig_tokens.str.len()\n",
    "\n",
    "def clean_book(url):\n",
    "\t# This gets a book by its gutenberg id number\n",
    "\tif pd.notna(url):\n",
    "\t\tpg_id = url.split('/pg')[-1].split('.')[0]\n",
    "\t\ttry:\n",
    "\t\t\traw_book = gutenbergpy.textget.get_text_by_id(pg_id) # with headers\n",
    "\t\t\tclean_book = gutenbergpy.textget.strip_headers(raw_book) # without headers\n",
    "\t\t\treturn clean_book\n",
    "\t\texcept Exception as e:\n",
    "\t\t\treturn None\n",
    "\n",
    "combined_novels_nyt_df.loc[:, 'cleaned_pg_eng_text'] = combined_novels_nyt_df.pg_eng_url.apply(clean_book)\n",
    "combined_novels_nyt_df.loc[:, 'cleaned_pg_orig_text'] = combined_novels_nyt_df.pg_orig_url.apply(clean_book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER Code\n",
    "\n",
    "There are many libraries for doing NER but today we'll be using `spaCy`, which is one of the most popular. You can visit the documentation here [https://spacy.io/usage](https://spacy.io/usage).\n",
    "\n",
    "Now spaCy is a much more complex library than we have used before so let's try out an example and then talk through some the documentation. Let's start with an example from their spaCy 101 guide [https://spacy.io/usage/spacy-101#annotations-ner](https://spacy.io/usage/spacy-101#annotations-ner) and try copying some code.\n",
    "\n",
    "```python\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "```\n",
    "\n",
    "In our case we'll change `nlp` to be `eng_nlp`. Let's try running this code and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = eng_nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what exactly does this output mean? Well first let's explore a bit of the spaCy API. First how could we check out what our `doc` variable contains?\n",
    "\n",
    "We should see `spacy.tokens.doc.Doc` so let's take a look at the `Doc` <https://spacy.io/api/doc>. The documentation is pretty dense, but we should get a sense that a `Doc` is a collection of [`Token`](https://spacy.io/api/token) classes, and that it has a number of built-in methods. \n",
    "\n",
    "We can list these methods with the following code:\n",
    "\n",
    "```python\n",
    "# Let's dig into what this Class gives us\n",
    "[prop for prop in dir(doc) if not prop.startswith('_')]\n",
    "\n",
    "first_word = doc[0]\n",
    "type(first_word)\n",
    "\n",
    "[prop for prop in dir(first_word) if not prop.startswith('__')]\n",
    "```\n",
    "\n",
    "Take a look at the [`similiarity`](https://spacy.io/api/doc#similarity) and [`ents`](https://spacy.io/api/doc#ents) methods. What do these methods/attributes do? \n",
    "\n",
    "Part of understanding what they are doing, requires understanding how spaCy works. Below is a figure of their pipeline:\n",
    "\n",
    "![spacy pipeline](https://d33wubrfki0l68.cloudfront.net/3ad0582d97663a1272ffc4ccf09f1c5b335b17e9/7f49c/pipeline-fde48da9b43661abcdf62ab70a546d71.svg)\n",
    "\n",
    "This gives us a bit of a sense of how this is working (recognize tokenizer?) but let's also read their broad overview page <https://spacy.io/usage/facts-figures>. \n",
    "\n",
    "Looking at their comparison usage, what do you think are the benefits and limitations of spaCy? How does spaCy create their models and what are these models exactly?\n",
    "\n",
    "Returning our example above, let's try using one of spaCy's built-in visualizations to understand how this is working.\n",
    "\n",
    "```python\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\")\n",
    "```\n",
    "\n",
    "We should now see something that looks like this:\n",
    "\n",
    "![spacy pos](https://spacy.io/images/displacy.svg)\n",
    "\n",
    "What this visualization is highlighting is essentially how spaCy works, which is with something called Parts of Speech Tagging.\n",
    "\n",
    "From the [spaCy docs](https://spacy.io/usage/linguistic-features#pos-tagging):\n",
    "\n",
    "> After tokenization, spaCy can parse and tag a given Doc. This is where the trained pipeline and its statistical models come in, which enable spaCy to make predictions of which tag or label most likely applies in this context. A trained component includes binary data that is produced by showing a system enough examples for it to make predictions that generalize across the language – for example, a word following “the” in English is most likely a noun.\n",
    "\n",
    "So the key thing to understand is that this super powerful library also comes with a lot of built-in assumptions about how language in your corpus is structured.\n",
    "\n",
    "Now that we've considered some of the pros and cons, let's try out spaCy's NER with some of our data.\n",
    "\n",
    "spaCy does have a limit on how much text it can process at once, so we'll need to break up our text into smaller chunks. Let's try a small subset first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152K)\\n\\n\\nFull 73 88 CARDINAL\n",
      "Miguel de Cervantes\\n\\n\\n\\n Translated 129 167 PERSON\n",
      "John Ormsby\\n\\n\\n\\n\\nEbook 171 197 PERSON\n",
      "Note\\n\\n\\n\\nThe 218 233 ORG\n",
      "Ormsby 320 326 PERSON\n",
      "J. W. Clark 391 402 PERSON\n",
      "Gustave Dor\\xc3\\xa9 419 438 PERSON\n",
      "Clark 440 445 PERSON\n",
      "English 491 498 LANGUAGE\n",
      "\\xe2\\x80\\x98Don Quixote\\xe2\\x80\\x99 507 542 ORG\n"
     ]
    }
   ],
   "source": [
    "text = combined_novels_nyt_df.cleaned_pg_eng_text[0][0:1000]\n",
    "doc = eng_nlp(text)\n",
    "\n",
    "for ent in doc.ents[0:10]:\n",
    "\tprint(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could visualize this using displacy as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">b'\\n\\n\\n\\nbookcover.jpg\\n\\n\\nFull Size\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nspine.jpg (\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    152K)\\n\\n\\nFull\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " Size\\n\\n\\n\\n\\n\\n\\nDon Quixote\\n\\n\\n\\nby \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Miguel de Cervantes\\n\\n\\n\\n Translated\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " by \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    John Ormsby\\n\\n\\n\\n\\nEbook\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " Editor\\xe2\\x80\\x99s \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Note\\n\\n\\n\\nThe\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " book cover and spine above and the images which follow were not\\npart of the original \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ormsby\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " translation\\xe2\\x80\\x94they are taken from the 1880\\nedition of \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    J. W. Clark\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", illustrated by \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Gustave Dor\\xc3\\xa9\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Clark\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " in his\\nedition states that, \\xe2\\x80\\x9cThe \n",
       "<mark class=\"entity\" style=\"background: #ff8197; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    English\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LANGUAGE</span>\n",
       "</mark>\n",
       " text of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    \\xe2\\x80\\x98Don Quixote\\xe2\\x80\\x99\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " adopted in this\\nedition is that of \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Jarvis\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", with occasional corrections from Motteaux.\\xe2\\x80\\x9d\\nSee in the introduction below \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    John\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " Ormsby\\xe2\\x80\\x99s critique of both the Jarvis\\nand \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Motteaux\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " translations. It has been elected in the present \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Project\\nGutenberg\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " edition to attach the famous engravings of \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Gustave Dor\\xc3\\xa9\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " to\\nthe \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ormsby\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " translation instead of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Jarvis/Motteaux\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". The detail of\\nmany of the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Dor\\xc3\\xa9\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " engravings can be fully appreci</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice there's some weird looking characters in the text, like `\\xe2\\x80\\x99s` or `\\n`. These are unicode characters and for example `\\n` is a newline character. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More importantly we are starting to see how NER works and what entities it identifies. Below we can see the number of entities that exist in spaCy. \n",
    "\n",
    "![ner entites](https://devopedia.org/images/article/256/8660.1580659054.png)\n",
    "\n",
    "We could for example be interested in what places are mentioned in these novels, so we would want to subset to only `GPE` entities. \n",
    "\n",
    "```python\n",
    "for ent in doc.ents:\n",
    "\tif ent.label_ == 'GPE':\n",
    "\t\tprint(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "```\n",
    "\n",
    "So let's try writing code that gets all the `GPE` entities from our novels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|██████████| 251/251 [02:01<00:00,  2.06it/s]\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, n=10000):\n",
    "    return (text[i:i+n] for i in range(0, len(text), n))\n",
    "\n",
    "def get_entities(text, nlp, n=10000, entity_type=\"GPE\"):\n",
    "    entities = []\n",
    "    chunks = list(chunk_text(text, n))\n",
    "    for chunk in tqdm(chunks, desc=\"Processing Text Chunks\"):\n",
    "        doc = nlp(chunk)\n",
    "        entities.extend(ent.text for ent in doc.ents if ent.label_ == entity_type)\n",
    "    return entities\n",
    "\n",
    "# Assuming combined_novels_nyt_df and eng_nlp are already defined\n",
    "test_df = combined_novels_nyt_df[0:1]\n",
    "# tqdm.pandas(desc=\"Identifying Entities\")\n",
    "test_df['pg_eng_gpe'] = test_df.cleaned_pg_eng_text.apply(get_entities, nlp=eng_nlp, n=10000, entity_type=\"GPE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-1bf9b1a9bbc94126bfff9e5aa8eb4437.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-1bf9b1a9bbc94126bfff9e5aa8eb4437.vega-embed details,\n",
       "  #altair-viz-1bf9b1a9bbc94126bfff9e5aa8eb4437.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-1bf9b1a9bbc94126bfff9e5aa8eb4437\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-1bf9b1a9bbc94126bfff9e5aa8eb4437\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-1bf9b1a9bbc94126bfff9e5aa8eb4437\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-8aede4961f73db8b8e6acd467d64803c\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"x\": {\"field\": \"counts\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"pg_eng_gpe\", \"sort\": \"-x\", \"type\": \"nominal\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-8aede4961f73db8b8e6acd467d64803c\": [{\"pg_eng_gpe\": \"Sancho\", \"counts\": 1343}, {\"pg_eng_gpe\": \"Dulcinea\", \"counts\": 161}, {\"pg_eng_gpe\": \"Spain\", \"counts\": 84}, {\"pg_eng_gpe\": \"Lothario\", \"counts\": 44}, {\"pg_eng_gpe\": \"Rocinante\", \"counts\": 40}, {\"pg_eng_gpe\": \"se\\\\xc3\\\\xb1or\", \"counts\": 36}, {\"pg_eng_gpe\": \"\\\\xe2\\\\x80\\\\x9cSe\\\\xc3\\\\xb1or\", \"counts\": 34}, {\"pg_eng_gpe\": \"Se\\\\xc3\\\\xb1or\", \"counts\": 31}, {\"pg_eng_gpe\": \"Quiteria\", \"counts\": 27}, {\"pg_eng_gpe\": \"Algiers\", \"counts\": 23}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explode the list of entities into separate rows\n",
    "exploded_test_df = test_df.explode('pg_eng_gpe')\n",
    "# group by the entities and count the number of times they appear\n",
    "gpe_counts = exploded_test_df.groupby('pg_eng_gpe').size().reset_index(name='counts')\n",
    "# sort the entities by the number of times they appear\n",
    "gpe_counts = gpe_counts.sort_values(by='counts', ascending=False)\n",
    "# plot the top 10 entities\n",
    "alt.Chart(gpe_counts[0:10]).mark_bar().encode(\n",
    "\ty=alt.Y('pg_eng_gpe', sort='-x'),\n",
    "\tx='counts'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are starting to see how NER might help us start to explore our cultural data. You'll notice that there is `se\\xc3\\xb1or` which is a unicode character for `ñ` and `c3` and `b1` are the hex values for that character, which means it stands for `senor`. It's unclear why spaCy is recognizing this as a `GPE` entity, but it's likely because of some issues in the model (remember we're using an English model). We could try experimenting with other spaCy models to see if this changes, but fundamentally this shows both the power and danger of relying on these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What to try next?\n",
    "\n",
    "1. Try using a different spaCy model and see how the results change.\n",
    "2. Try running it on the full dataset and see what you find.\n",
    "3. Try spaCy's parts of speech tagging and see if you can find any interesting patterns, like which pronouns are most common in these novels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run TF-IDF we'll be using the `scikit-learn` library. You can find the documentation here [https://scikit-learn.org/stable/](https://scikit-learn.org/stable/). In particular, we'll be using the `TfidfVectorizer` class [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>term</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115304890</th>\n",
       "      <td>The Last Days of Pompeii</td>\n",
       "      <td>the</td>\n",
       "      <td>0.730242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115304767</th>\n",
       "      <td>Germinal</td>\n",
       "      <td>the</td>\n",
       "      <td>0.700247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115304868</th>\n",
       "      <td>Nostromo</td>\n",
       "      <td>the</td>\n",
       "      <td>0.699570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115304932</th>\n",
       "      <td>Death In Venice</td>\n",
       "      <td>the</td>\n",
       "      <td>0.687305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115304523</th>\n",
       "      <td>The Grapes of Wrath</td>\n",
       "      <td>the</td>\n",
       "      <td>0.670729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115304601</th>\n",
       "      <td>The War of the Worlds</td>\n",
       "      <td>the</td>\n",
       "      <td>0.668319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115304796</th>\n",
       "      <td>The Phantom of the Opera</td>\n",
       "      <td>the</td>\n",
       "      <td>0.667619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115304750</th>\n",
       "      <td>Death Comes for the Archbishop</td>\n",
       "      <td>the</td>\n",
       "      <td>0.659896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115304525</th>\n",
       "      <td>The Last of the Mohicans</td>\n",
       "      <td>the</td>\n",
       "      <td>0.659860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115304564</th>\n",
       "      <td>A Journey to the Center of the Earth</td>\n",
       "      <td>the</td>\n",
       "      <td>0.653512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title term     score\n",
       "115304890              The Last Days of Pompeii  the  0.730242\n",
       "115304767                              Germinal  the  0.700247\n",
       "115304868                              Nostromo  the  0.699570\n",
       "115304932                       Death In Venice  the  0.687305\n",
       "115304523                   The Grapes of Wrath  the  0.670729\n",
       "115304601                 The War of the Worlds  the  0.668319\n",
       "115304796              The Phantom of the Opera  the  0.667619\n",
       "115304750        Death Comes for the Archbishop  the  0.659896\n",
       "115304525              The Last of the Mohicans  the  0.659860\n",
       "115304564  A Journey to the Center of the Earth  the  0.653512"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the text data\n",
    "tfidf_matrix = vectorizer.fit_transform(combined_novels_nyt_df.cleaned_pg_eng_text.fillna(''))\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame for better readability\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Add the titles to the DataFrame\n",
    "tfidf_df['title'] = combined_novels_nyt_df['title'].values\n",
    "\n",
    "# Melt the DataFrame to get a long format DataFrame with terms and scores\n",
    "melted_tfidf_df = tfidf_df.melt(id_vars=['title'], var_name='term', value_name='score')\n",
    "\n",
    "# Sort the DataFrame by score in descending order\n",
    "sorted_tfidf_df = melted_tfidf_df.sort_values(by='score', ascending=False)\n",
    "\n",
    "# Display the top 10 results\n",
    "sorted_tfidf_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that our top results are all the words `the`. This is exactly the issue with common words and why stop words are so popular. So we can try removing the stop words and see what results we get.\n",
    "\n",
    "Indeed, TF-IDF has a number of parameters that you can set. Specifically it includes parameters for dealing with stop words, max_df, min_df, and ngram_range. Let's experiment with some of these parameters.\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Let's try out some different parameters\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_df=0.5, min_df=2, ngram_range=(1, 2))\n",
    "tfidf_matrix = tfidf.fit_transform(df['text'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>term</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38042168</th>\n",
       "      <td>Heidi</td>\n",
       "      <td>heidi</td>\n",
       "      <td>0.821331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107126913</th>\n",
       "      <td>A Christmas Carol</td>\n",
       "      <td>scrooge</td>\n",
       "      <td>0.819125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8154752</th>\n",
       "      <td>This Side of Paradise</td>\n",
       "      <td>amory</td>\n",
       "      <td>0.786431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16185827</th>\n",
       "      <td>Candide</td>\n",
       "      <td>candide</td>\n",
       "      <td>0.748896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65341057</th>\n",
       "      <td>Resurrection</td>\n",
       "      <td>nekhludoff</td>\n",
       "      <td>0.732123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124364958</th>\n",
       "      <td>Heart of Darkness</td>\n",
       "      <td>x80</td>\n",
       "      <td>0.684902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141660306</th>\n",
       "      <td>Heart of Darkness</td>\n",
       "      <td>xe2</td>\n",
       "      <td>0.684902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141659937</th>\n",
       "      <td>Uncle Tom's Cabin</td>\n",
       "      <td>xe2</td>\n",
       "      <td>0.684411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124364589</th>\n",
       "      <td>Uncle Tom's Cabin</td>\n",
       "      <td>x80</td>\n",
       "      <td>0.684411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124364572</th>\n",
       "      <td>The Adventures of Tom Sawyer</td>\n",
       "      <td>x80</td>\n",
       "      <td>0.684342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  title        term     score\n",
       "38042168                          Heidi       heidi  0.821331\n",
       "107126913             A Christmas Carol     scrooge  0.819125\n",
       "8154752           This Side of Paradise       amory  0.786431\n",
       "16185827                        Candide     candide  0.748896\n",
       "65341057                   Resurrection  nekhludoff  0.732123\n",
       "124364958             Heart of Darkness         x80  0.684902\n",
       "141660306             Heart of Darkness         xe2  0.684902\n",
       "141659937             Uncle Tom's Cabin         xe2  0.684411\n",
       "124364589             Uncle Tom's Cabin         x80  0.684411\n",
       "124364572  The Adventures of Tom Sawyer         x80  0.684342"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Fit and transform the text data\n",
    "tfidf_matrix = vectorizer.fit_transform(combined_novels_nyt_df.cleaned_pg_eng_text.fillna(''))\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame for better readability\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Add the titles to the DataFrame\n",
    "tfidf_df['title'] = combined_novels_nyt_df['title'].values\n",
    "\n",
    "# Melt the DataFrame to get a long format DataFrame with terms and scores\n",
    "melted_tfidf_df = tfidf_df.melt(id_vars=['title'], var_name='term', value_name='score')\n",
    "\n",
    "# Sort the DataFrame by score in descending order\n",
    "sorted_tfidf_no_stopwords_df = melted_tfidf_df.sort_values(by='score', ascending=False)\n",
    "\n",
    "# Display the top 10 results\n",
    "sorted_tfidf_no_stopwords_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see we are getting words that are more specific to each novel, though we still have some unicode characters like `x80` and `x99` which are `€` and `’` respectively. We could try removing these characters from our text or use more of the TF-IDF parameters to make them less important. We can also add a custom token pattern that removes these characters.\n",
    "\n",
    "```python\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_df=0.5, min_df=2, ngram_range=(1, 2), token_pattern=r'(?u)\\b\\w+\\b')\n",
    "tfidf_matrix = tfidf.fit_transform(df['text'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively though we could actually use spaCy to help us clean this text. Let's try using spaCy to clean our text and see if we get better results.\n",
    "\n",
    "```python\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a function to clean the text with spaCy\n",
    "def clean_text_spacy(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    # Remove stopwords, punctuation, and non-alphabetic tokens\n",
    "    tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "    # Join tokens back to a single string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the cleaning function to the text data\n",
    "combined_novels_nyt_df['cleaned_text'] = combined_novels_nyt_df['cleaned_pg_eng_text'].fillna('').apply(clean_text_spacy)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n=10000):\n",
    "    return (text[i:i+n] for i in range(0, len(text), n))\n",
    "\n",
    "# Define a function to clean text with spaCy, processing in chunks\n",
    "def clean_text_spacy(text, nlp, chunk_size=10000):\n",
    "    cleaned_chunks = []\n",
    "    chunks = list(chunk_text(text, chunk_size))\n",
    "    \n",
    "    # Chunk the text and process each chunk\n",
    "    for chunk in tqdm(chunks, desc=\"Processing Chunks\", leave=False):\n",
    "        doc = nlp(chunk)  # Process each chunk with spaCy\n",
    "        # Remove stopwords, punctuation, and non-alphabetic tokens, and apply lemmatization\n",
    "        tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "        # Join tokens of the current chunk and add to cleaned chunks\n",
    "        cleaned_chunks.append(' '.join(tokens))\n",
    "    \n",
    "    # Join all chunks into a single string (optional, based on your needs)\n",
    "    return ' '.join(cleaned_chunks)\n",
    "\n",
    "# Assuming combined_novels_nyt_df and eng_nlp are already defined\n",
    "tqdm.pandas(desc=\"Cleaning Text\")\n",
    "combined_novels_nyt_df['cleaned_pg_eng_text_spacy'] = combined_novels_nyt_df.cleaned_pg_eng_text.fillna('').progress_apply(clean_text_spacy, nlp=eng_nlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>term</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44672498</th>\n",
       "      <td>The Hunchback of Notre Dame</td>\n",
       "      <td>n</td>\n",
       "      <td>0.992497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44672555</th>\n",
       "      <td>The Hound of the Baskervilles</td>\n",
       "      <td>n</td>\n",
       "      <td>0.988642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44672495</th>\n",
       "      <td>The Count of Monte Cristo</td>\n",
       "      <td>n</td>\n",
       "      <td>0.951079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44672602</th>\n",
       "      <td>The Sun Also Rises</td>\n",
       "      <td>n</td>\n",
       "      <td>0.950076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44672542</th>\n",
       "      <td>Sons and Lovers</td>\n",
       "      <td>n</td>\n",
       "      <td>0.946033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44672446</th>\n",
       "      <td>Jane Eyre</td>\n",
       "      <td>n</td>\n",
       "      <td>0.944493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44672469</th>\n",
       "      <td>Les Misérables</td>\n",
       "      <td>n</td>\n",
       "      <td>0.941966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44672520</th>\n",
       "      <td>A Journey to the Center of the Earth</td>\n",
       "      <td>n</td>\n",
       "      <td>0.941441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44672492</th>\n",
       "      <td>Ulysses</td>\n",
       "      <td>n</td>\n",
       "      <td>0.939559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44672491</th>\n",
       "      <td>Ulysses</td>\n",
       "      <td>n</td>\n",
       "      <td>0.939559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title term     score\n",
       "44672498           The Hunchback of Notre Dame    n  0.992497\n",
       "44672555         The Hound of the Baskervilles    n  0.988642\n",
       "44672495             The Count of Monte Cristo    n  0.951079\n",
       "44672602                    The Sun Also Rises    n  0.950076\n",
       "44672542                       Sons and Lovers    n  0.946033\n",
       "44672446                             Jane Eyre    n  0.944493\n",
       "44672469                        Les Misérables    n  0.941966\n",
       "44672520  A Journey to the Center of the Earth    n  0.941441\n",
       "44672492                               Ulysses    n  0.939559\n",
       "44672491                               Ulysses    n  0.939559"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\", min_df=1, max_df=0.7,)\n",
    "\n",
    "# Fit and transform the text data\n",
    "tfidf_matrix = vectorizer.fit_transform(combined_novels_nyt_df.cleaned_pg_eng_text_spacy.fillna(''))\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame for better readability\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Add the titles to the DataFrame\n",
    "tfidf_df['title'] = combined_novels_nyt_df['title'].values\n",
    "\n",
    "# Melt the DataFrame to get a long format DataFrame with terms and scores\n",
    "melted_tfidf_df = tfidf_df.melt(id_vars=['title'], var_name='term', value_name='score')\n",
    "\n",
    "# Sort the DataFrame by score in descending order\n",
    "sorted_tfidf_no_stopwords_min_max_df = melted_tfidf_df.sort_values(by='score', ascending=False)\n",
    "\n",
    "# Display the top 10 results\n",
    "sorted_tfidf_no_stopwords_min_max_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that I've set the parameter `min_df=1` and `max_df=0.7`. If we go to the scikit-learn documentation for `TfidfVectorizer` we can see that `min_df` is the minimum document frequency and `max_df` is the maximum document frequency. This means that we are only including words that appear in at least 2 documents and in at most 70% of the documents. So you can imagine that these parameters can determine a lot of our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\", min_df=1, max_df=0.9, ngram_range=(1, 2), max_features=1000)\n",
    "\n",
    "# Fit and transform the text data\n",
    "tfidf_matrix = vectorizer.fit_transform(combined_novels_nyt_df.cleaned_pg_eng_text_spacy.fillna(''))\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame for better readability\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Add the titles to the DataFrame\n",
    "tfidf_df['title'] = combined_novels_nyt_df['title'].values\n",
    "\n",
    "# Melt the DataFrame to get a long format DataFrame with terms and scores\n",
    "melted_tfidf_df = tfidf_df.melt(id_vars=['title'], var_name='term', value_name='score')\n",
    "\n",
    "# Sort the DataFrame by score in descending order\n",
    "sorted_tfidf_no_stopwords_min_max_ngram_df = melted_tfidf_df.sort_values(by='score', ascending=False)\n",
    "\n",
    "# Display the top 10 results\n",
    "sorted_tfidf_no_stopwords_min_max_ngram_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-work-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
